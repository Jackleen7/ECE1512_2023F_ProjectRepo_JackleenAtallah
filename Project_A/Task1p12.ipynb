{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WYMfvCNPwpm"
   },
   "source": [
    "# Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vA8ppgB2P0aJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Union\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "builder = tfds.builder('mnist')\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 12\n",
    "NUM_CLASSES = 10  # 10 total classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2EFLQROP2R7"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ynByMG_UP4A4"
   },
   "outputs": [],
   "source": [
    "# Load train and test splits.\n",
    "def preprocess(x):\n",
    "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
    "  subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
    "  return image, subclass_labels\n",
    "\n",
    "\n",
    "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
    "mnist_train = mnist_train.map(preprocess)\n",
    "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
    "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "mnist_test = tfds.load('mnist', split='test').cache()\n",
    "mnist_test = mnist_test.map(preprocess).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAZwfvW5P63q"
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zINgDkA7P7BP"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Build CNN teacher.\n",
    "cnn_model = tf.keras.Sequential()\n",
    "\n",
    "# your code start from here for stpe 2\n",
    "cnn_model.add(tf.keras.layers.Conv2D(filters = 32,kernel_size = (3,3),\n",
    "                                     strides=(1, 1),activation ='relu'))\n",
    "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2),strides =(1,1)))\n",
    "cnn_model.add(tf.keras.layers.Conv2D(filters = 64,kernel_size = (3,3),\n",
    "                                     strides=(1, 1),activation ='relu'))\n",
    "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2),strides =(2,2)))\n",
    "cnn_model.add(tf.keras.layers.Flatten())\n",
    "cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
    "cnn_model.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
    "cnn_model.add(tf.keras.layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implement intermediate teacher model (TA model)\n",
    "cnn_ta_model = tf.keras.Sequential()\n",
    "\n",
    "# your code start from here for stpe 2\n",
    "cnn_ta_model.add(tf.keras.layers.Conv2D(filters = 16, kernel_size = (3,3),\n",
    "            strides=(1, 1), activation ='relu', input_shape = (28, 28, 1)))\n",
    "cnn_ta_model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2),strides =(1,1)))\n",
    "cnn_ta_model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3),\n",
    "            strides=(1, 1), activation ='relu'))\n",
    "cnn_ta_model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2),strides =(2,2)))\n",
    "cnn_ta_model.add(tf.keras.layers.Flatten())\n",
    "cnn_ta_model.add(tf.keras.layers.Dropout(0.25))\n",
    "cnn_ta_model.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "cnn_ta_model.add(tf.keras.layers.Dropout(0.25))\n",
    "cnn_ta_model.add(tf.keras.layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fully connected student.\n",
    "fc_model = tf.keras.Sequential()\n",
    "\n",
    "# your code start from here for step 2\n",
    "fc_model.add(tf.keras.layers.Flatten())\n",
    "fc_model.add(tf.keras.layers.Dense(784,activation = 'relu'))\n",
    "fc_model.add(tf.keras.layers.Dense(784,activation = 'relu'))\n",
    "fc_model.add(tf.keras.layers.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JWGucyrQGav"
   },
   "source": [
    "# Teacher loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhzBP6ZLQJ57"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_teacher_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation teacher loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  subclass_logits = cnn_model(images, training=True)\n",
    "\n",
    "  # Compute cross-entropy loss for subclasses.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "  cross_entropy_loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( \\\n",
    "          labels, subclass_logits))\n",
    "\n",
    "\n",
    "  return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS8xkuH0QbOS"
   },
   "source": [
    "# TA loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lDKia4gPQMIr"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Hyperparameters for distillation (need to be tuned).\n",
    "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
    "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
    "\n",
    "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
    "                      temperature: Union[float, tf.Tensor]):\n",
    "  \"\"\"Compute distillation loss.\n",
    "\n",
    "  This function computes cross entropy between softened logits and softened\n",
    "  targets. The resulting loss is scaled by the squared temperature so that\n",
    "  the gradient magnitude remains approximately constant as the temperature is\n",
    "  changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
    "  a neural network.\"\n",
    "\n",
    "  Args:\n",
    "    teacher_logits: A Tensor of logits provided by the teacher.\n",
    "    student_logits: A Tensor of logits provided by the student, of the same\n",
    "      shape as `teacher_logits`.\n",
    "    temperature: Temperature to use for distillation.\n",
    "\n",
    "  Returns:\n",
    "    A scalar Tensor containing the distillation loss.\n",
    "  \"\"\"\n",
    " # your code start from here for step 3\n",
    "  soft_targets = tf.nn.softmax(teacher_logits/temperature, axis = -1)\n",
    "\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TA loss\n",
    "def compute_ta_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  ta_subclass_logits = cnn_ta_model(images, training=True)\n",
    "\n",
    "  # Compute subclass distillation loss between student subclass logits and\n",
    "  # softened teacher subclass targets probabilities.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  teacher_subclass_logits = cnn_model(images, training=False)\n",
    "  distillation_loss_value = distillation_loss(teacher_subclass_logits,\n",
    "                ta_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  # Compute cross-entropy loss with hard targets.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  cross_entropy_loss_value = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels, ta_subclass_logits))\n",
    "\n",
    "  return ALPHA*distillation_loss_value + (1 - ALPHA)*cross_entropy_loss_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27CU96lqOE5w"
   },
   "source": [
    "# Student loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "utynMp7VN-mZ"
   },
   "outputs": [],
   "source": [
    "def compute_student_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  student_subclass_logits = fc_model(images, training=True)\n",
    "\n",
    "  # Compute subclass distillation loss between student subclass logits and\n",
    "  # softened teacher subclass targets probabilities.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  ta_subclass_logits = cnn_ta_model(images, training=False)\n",
    "  ta_distillation_loss_value = distillation_loss(ta_subclass_logits,\n",
    "                student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  # teacher_subclass_logits = cnn_model(images, training=False)\n",
    "  # teacher_distillation_loss_value = distillation_loss(teacher_subclass_logits,\n",
    "  #               student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  # Compute cross-entropy loss with hard targets.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  cross_entropy_loss_value = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels, student_subclass_logits))\n",
    "\n",
    "  return ALPHA*ta_distillation_loss_value + (1 - ALPHA)*cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJ1uyvurQ3w4"
   },
   "source": [
    "# Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EtoLbp8uQ4Vl"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_num_correct(model, images, labels):\n",
    "  \"\"\"Compute number of correctly classified images in a batch.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Number of correctly classified images.\n",
    "  \"\"\"\n",
    "  class_logits = model(images, training=False)\n",
    "  return tf.reduce_sum(\n",
    "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
    "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn):\n",
    "  \"\"\"Perform training and evaluation for a given model.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    compute_loss_fn: A function that computes the training loss given the\n",
    "      images, and labels.\n",
    "  \"\"\"\n",
    "\n",
    "  # your code start from here for step 4\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "  for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Run training.\n",
    "    print('Epoch {}: '.format(epoch), end='')\n",
    "    for images, labels in mnist_train:\n",
    "      with tf.GradientTape() as tape:\n",
    "         # your code start from here for step 4\n",
    "\n",
    "        loss_value = compute_loss_fn(images, labels)\n",
    "\n",
    "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Run evaluation.\n",
    "    num_correct = 0\n",
    "    num_total = builder.info.splits['test'].num_examples\n",
    "    for images, labels in mnist_test:\n",
    "      # your code start from here for step 4\n",
    "      num_correct_batch, pred_digit, true_digit = compute_num_correct(model, images, labels)\n",
    "      num_correct += num_correct_batch\n",
    "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
    "        num_correct / num_total * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQL1lJdaRPT1"
   },
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6JMUG_uMHJd"
   },
   "source": [
    "Do the training and evaluation as follows\n",
    "*   Train teacher (this was already done in previous points)\n",
    "*   Train TA as a student w.r.t to the teacher\n",
    "*   Train student as student w.r.t to TA (as teacher)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356219,
     "status": "ok",
     "timestamp": 1698096220444,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "-AGHbyABRPz3",
    "outputId": "30d7eeaa-f75e-4561-cde6-0c42889c934a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Teacher Model\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load Teaher Model From previous problems\n",
    "print('Loading Teacher Model')\n",
    "cnn_model = load_model('Teacher_Model_Task1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Evaluating TA WRT to teacher model at ALPHA = 0.5 T = 4.0\n",
      "Epoch 1: Class_accuracy: 97.72%\n",
      "Epoch 2: Class_accuracy: 98.50%\n",
      "Epoch 3: Class_accuracy: 98.75%\n",
      "Epoch 4: Class_accuracy: 98.85%\n",
      "Epoch 5: Class_accuracy: 98.97%\n",
      "Epoch 6: Class_accuracy: 99.02%\n",
      "Epoch 7: Class_accuracy: 99.06%\n",
      "Epoch 8: Class_accuracy: 99.06%\n",
      "Epoch 9: Class_accuracy: 99.16%\n",
      "Epoch 10: Class_accuracy: 99.13%\n",
      "Epoch 11: Class_accuracy: 99.14%\n",
      "Epoch 12: Class_accuracy: 99.16%\n"
     ]
    }
   ],
   "source": [
    "# TA training\n",
    "print('Training and Evaluating TA WRT to teacher model at ALPHA = ' + str(ALPHA) + \\\n",
    "      ' T = ' + str(DISTILLATION_TEMPERATURE))\n",
    "train_and_evaluate(cnn_ta_model, compute_ta_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Evaluating Student Model WRT to TA model at ALPHA = 0.5 T = 4.0\n",
      "Epoch 1: Class_accuracy: 96.63%\n",
      "Epoch 2: Class_accuracy: 97.87%\n",
      "Epoch 3: Class_accuracy: 98.16%\n",
      "Epoch 4: Class_accuracy: 98.45%\n",
      "Epoch 5: Class_accuracy: 98.55%\n",
      "Epoch 6: Class_accuracy: 98.41%\n",
      "Epoch 7: Class_accuracy: 98.63%\n",
      "Epoch 8: Class_accuracy: 98.62%\n",
      "Epoch 9: Class_accuracy: 98.65%\n",
      "Epoch 10: Class_accuracy: 98.60%\n",
      "Epoch 11: Class_accuracy: 98.67%\n",
      "Epoch 12: Class_accuracy: 98.70%\n"
     ]
    }
   ],
   "source": [
    "# Student Training\n",
    "print('Training and Evaluating Student Model WRT to TA model at ALPHA = ' + str(ALPHA) + \\\n",
    "      ' T = ' + str(DISTILLATION_TEMPERATURE))\n",
    "train_and_evaluate(fc_model, compute_student_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code start from here for step 8\n",
    "## https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-849439287\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "#print('TensorFlow:', tf.__version__)\n",
    "def get_flops_number(model):\n",
    "  forward_pass = tf.function(model.call,\n",
    "      input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\n",
    "\n",
    "  graph_info = profile(forward_pass.get_concrete_function().graph,\n",
    "                        options=ProfileOptionBuilder.float_operation())\n",
    "\n",
    "  # The //2 is necessary since `profile` counts multiply and accumulate\n",
    "  # as two flops, here we report the total number of multiply accumulate ops\n",
    "  flops = graph_info.total_float_ops // 2\n",
    "  return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 25, 25, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 23, 23, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 11, 11, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7744)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7744)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               991360    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,011,466\n",
      "Trainable params: 1,011,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Jackyy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:5250: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jackyy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:5250: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Flops: 11,021,029\n"
     ]
    }
   ],
   "source": [
    "#### Teacher Model Architecture #######\n",
    "cnn_model.summary()\n",
    "teacher_flops = get_flops_number(cnn_model)\n",
    "print('Teacher Flops: {:,}'.format(teacher_flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 25, 25, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 23, 23, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 11, 11, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3872)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3872)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               495744    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 501,834\n",
      "Trainable params: 501,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "TA Flops: 3,073,557\n"
     ]
    }
   ],
   "source": [
    "#### Teacher Assistant Model Architecture #######\n",
    "cnn_ta_model.summary()\n",
    "ta_flops = get_flops_number(cnn_ta_model)\n",
    "print('TA Flops: {:,}'.format(ta_flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 784)               615440    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 784)               615440    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,238,730\n",
      "Trainable params: 1,238,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Student Flops: 1,237,941\n"
     ]
    }
   ],
   "source": [
    "#### Student Model Architecture #######\n",
    "fc_model.summary()\n",
    "student_flops = get_flops_number(fc_model)\n",
    "print('Student Flops: {:,}'.format(student_flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
