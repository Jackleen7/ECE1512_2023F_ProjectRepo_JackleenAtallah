{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WYMfvCNPwpm"
   },
   "source": [
    "# Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1698677350336,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "vA8ppgB2P0aJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Union\n",
    "import csv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "split_data = False   ## Use it only once to split data into train/test using annotation file\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_EPOCHS = 10\n",
    "FINE_TUNE_EPOCHS = 25\n",
    "NUM_CLASSES = 2\n",
    "run_type = 'local'  ## 'local' or 'colab'\n",
    "train_ta = True\n",
    "model = 'B3'    ## TA model selection 'B3' or 'B2' or 'D121'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16467,
     "status": "ok",
     "timestamp": 1698677368875,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "BxtRbf2Rmhni",
    "outputId": "16628bdd-0ad4-4ec9-8480-a6921426c895"
   },
   "outputs": [],
   "source": [
    "if run_type == 'local':\n",
    "    data_path = 'G:/My Drive/Colab Notebooks/MHIST/'\n",
    "else:\n",
    "    # Giving Access to Google Drive for loading data\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    data_path = '/content/drive/My Drive/Colab Notebooks/MHIST/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2EFLQROP2R7"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10486,
     "status": "ok",
     "timestamp": 1698677381280,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "D8UEq0Se4MtI",
    "outputId": "2c3dfac1-1322-4a77-b6c6-72c382e2af06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='image_folder',\n",
      "    full_name='image_folder/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Generic image classification dataset.\n",
      "    \"\"\",\n",
      "    homepage='https://www.tensorflow.org/datasets/catalog/image_folder',\n",
      "    data_dir='/root/tensorflow_datasets/image_folder/1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=Unknown size,\n",
      "    dataset_size=Unknown size,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(224, 224, 3), dtype=uint8),\n",
      "        'image/filename': Text(shape=(), dtype=string),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=977, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=2175, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(output_dir):\n",
    "  import shutil\n",
    "\n",
    "  file_name = 'annotations.csv'\n",
    "  ## Create directories for output train/test\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  os.makedirs(output_dir + 'train/HP/', exist_ok=True)\n",
    "  os.makedirs(output_dir + 'train/SSA/', exist_ok=True)\n",
    "  os.makedirs(output_dir + 'test/HP/', exist_ok=True)\n",
    "  os.makedirs(output_dir + 'test/SSA/', exist_ok=True)\n",
    "\n",
    "  with open(data_path + file_name) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    for line, row in enumerate(csv_reader):\n",
    "        if line > 0:\n",
    "          print('[INFO] Image number:' + str(line) + ' - Image Name: ' + row[0])\n",
    "          src = data_path + 'images_unzip/' + row[0]\n",
    "          dst = output_dir + row[3] + '/' + row[1] + '/' + row[0]\n",
    "          shutil.copy(src, dst)\n",
    "\n",
    "if split_data:\n",
    "  # Just do it once for data building\n",
    "  train_test_split(data_path + 'images_split/')\n",
    "\n",
    "builder = tfds.ImageFolder(data_path + 'images_split/', shape = (224, 224, 3))\n",
    "print(builder.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3175,
     "status": "ok",
     "timestamp": 1698677392996,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "zDwkQPmnCx1P"
   },
   "outputs": [],
   "source": [
    "# Load train and test splits.\n",
    "def preprocess(x):\n",
    "  image, label = x['image'], x['label']\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  subclass_labels = tf.one_hot(label, builder.info.features['label'].num_classes)\n",
    "\n",
    "\n",
    "  return image, subclass_labels\n",
    "\n",
    "\n",
    "mhist_train = builder.as_dataset(split='train', shuffle_files=False).cache()\n",
    "mhist_train = mhist_train.map(preprocess)\n",
    "mhist_train = mhist_train.shuffle(builder.info.splits['train'].num_examples)\n",
    "mhist_train = mhist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "mhist_test = builder.as_dataset(split='test').cache()\n",
    "mhist_test = mhist_test.map(preprocess).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1H66mQVeP-n"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1698677395431,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "2U9ALHzkTkSZ"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (224, 224, 3)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1698677397020,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "kZ9pypsReSfd"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 8\n",
    "## https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-849439287\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "#print('TensorFlow:', tf.__version__)\n",
    "def get_flops_number(model):\n",
    "  forward_pass = tf.function(model.call,\n",
    "      input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\n",
    "\n",
    "  graph_info = profile(forward_pass.get_concrete_function().graph,\n",
    "                        options=ProfileOptionBuilder.float_operation())\n",
    "\n",
    "  # The //2 is necessary since `profile` counts multiply and accumulate\n",
    "  # as two flops, here we report the total number of multiply accumulate ops\n",
    "  flops = graph_info.total_float_ops // 2\n",
    "  return flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAZwfvW5P63q"
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5170,
     "status": "ok",
     "timestamp": 1698677403848,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "zINgDkA7P7BP",
    "outputId": "bda1ff7b-67f9-4452-db0f-cfaba6c4564b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained teacher model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score and class accuracy for the last five epochs\n",
      "Teacher test F1 score = 0.7882733316229258\n",
      "Teacher test class accuracy = 85.322426\n",
      "######### Teacher Model, Number of Parameters, Flops ################\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequent  (None, 224, 224, 3)       0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 7, 7, 2048)        23564800  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 2048)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25665026 (97.90 MB)\n",
      "Trainable params: 22660098 (86.44 MB)\n",
      "Non-trainable params: 3004928 (11.46 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:140: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Flops: 3,489,281,549\n"
     ]
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "################## Teacher model  ###############################\n",
    "## We load Teacher model trained from previous problems\n",
    "print('Load pre-trained teacher model')\n",
    "if run_type == 'local':\n",
    "  teacher_model = load_model('Teacher_Model_Task2.h5')\n",
    "  hist = np.load('Teacher_Model_Task2_hist.npz')\n",
    "else:\n",
    "  teacher_model = load_model('/content/drive/My Drive/Colab Notebooks/Teacher_Model_Task2.h5')\n",
    "  hist = np.load('/content/drive/My Drive/Colab Notebooks/Teacher_Model_Task2_hist.npz')\n",
    "\n",
    "print('Average F1 score and class accuracy for the last five epochs')\n",
    "print('Teacher test F1 score = ' + str(np.mean(hist['f1_hist'][-5:])))\n",
    "print('Teacher test class accuracy = ' + str(np.mean(hist['acc_hist'][-5:])))\n",
    "\n",
    "print('######### Teacher Model, Number of Parameters, Flops ################')\n",
    "teacher_model.summary()\n",
    "teacher_flops = get_flops_number(teacher_model)\n",
    "print('Teacher Flops: {:,}'.format(teacher_flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7957,
     "status": "ok",
     "timestamp": 1698677421713,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "tgLXrRTZxoDF",
    "outputId": "71c82bc6-67ea-4ad2-d322-eea3f891b2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### TA Model, Number of Parameters, Flops ################\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_27 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " effecientnetB3 (Functional)  (None, 7, 7, 1536)       10783528  \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 1536)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,571,498\n",
      "Trainable params: 787,970\n",
      "Non-trainable params: 10,783,528\n",
      "_________________________________________________________________\n",
      "TA Flops: 978,528,889\n",
      "Number of layers in Base Model = 382\n"
     ]
    }
   ],
   "source": [
    "## TA Model (we choose an intermediate model EfficientNetB3)\n",
    "if model == 'B3':\n",
    "  ta_base = tf.keras.applications.EfficientNetB3(include_top=False,\n",
    "                                  weights=\"imagenet\", input_shape=IMG_SHAPE)\n",
    "  ## Remove the first processing units (Normalization/rescaling)\n",
    "  new_input = ta_base.layers[4].input\n",
    "  new_output = ta_base.layers[-1].output\n",
    "  # Create a new model starting from the fourth layer\n",
    "  ta_base = tf.keras.Model(inputs=new_input, outputs=new_output, name = 'effecientnetB3')\n",
    "elif model == 'B2':\n",
    "  ta_base = tf.keras.applications.EfficientNetB2(include_top=False,\n",
    "            weights=\"imagenet\", input_shape=IMG_SHAPE)\n",
    "  ## Remove the first processing units (Normalization/rescaling)\n",
    "  new_input = ta_base.layers[4].input\n",
    "  new_output = ta_base.layers[-1].output\n",
    "  # Create a new model starting from the fourth layer\n",
    "  ta_base = tf.keras.Model(inputs=new_input, outputs=new_output, name = 'effecientnetB2')\n",
    "elif model == 'D121':\n",
    "  ta_base = tf.keras.applications.DenseNet121(include_top=False,\n",
    "            weights=\"imagenet\", input_shape=IMG_SHAPE)\n",
    "\n",
    "\n",
    "\n",
    "ta_base.trainable = False\n",
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = data_augmentation(inputs)\n",
    "x = ta_base(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.15)(x)\n",
    "logits = tf.keras.layers.Dense(NUM_CLASSES)(x)\n",
    "ta_model = tf.keras.Model(inputs, logits)\n",
    "\n",
    "print('######### TA Model, Number of Parameters, Flops ################')\n",
    "ta_model.summary()\n",
    "ta_flops = get_flops_number(ta_model)\n",
    "print('TA Flops: {:,}'.format(ta_flops))\n",
    "print('Number of layers in Base Model = ' + str(len(ta_base.layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3867,
     "status": "ok",
     "timestamp": 1698677445509,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "NJdlq8g-xrGw",
    "outputId": "be0c82ad-409b-43e2-eda7-d4e6cea3e692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n",
      "######### Student Model, Number of Parameters, Flops ################\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1280)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               655872    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2914882 (11.12 MB)\n",
      "Trainable params: 656898 (2.51 MB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n",
      "Student Flops: 300,516,269\n"
     ]
    }
   ],
   "source": [
    "## Student Model\n",
    "# load trained base model\n",
    "mobilenet_base = tf.keras.applications.MobileNetV2(include_top=False,\n",
    "    input_shape=IMG_SHAPE, weights=\"imagenet\")\n",
    "mobilenet_layers_num = len(mobilenet_base.layers)\n",
    "\n",
    "mobilenet_base.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = data_augmentation(inputs)\n",
    "x = mobilenet_base(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "logits = tf.keras.layers.Dense(NUM_CLASSES)(x)\n",
    "\n",
    "# Build teacher model with inputs and outputs\n",
    "student_model = tf.keras.Model(inputs, logits)\n",
    "\n",
    "print('######### Student Model, Number of Parameters, Flops ################')\n",
    "student_model.summary()\n",
    "student_flops = get_flops_number(student_model)\n",
    "print('Student Flops: {:,}'.format(student_flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjyh94RM3srW"
   },
   "source": [
    "# Distillation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1698677449646,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "N-zrE8Uq3rBY"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Hyperparameters for distillation (need to be tuned).\n",
    "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
    "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
    "\n",
    "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
    "                      temperature: Union[float, tf.Tensor]):\n",
    "  \"\"\"Compute distillation loss.\n",
    "\n",
    "  This function computes cross entropy between softened logits and softened\n",
    "  targets. The resulting loss is scaled by the squared temperature so that\n",
    "  the gradient magnitude remains approximately constant as the temperature is\n",
    "  changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
    "  a neural network.\"\n",
    "\n",
    "  Args:\n",
    "    teacher_logits: A Tensor of logits provided by the teacher.\n",
    "    student_logits: A Tensor of logits provided by the student, of the same\n",
    "      shape as `teacher_logits`.\n",
    "    temperature: Temperature to use for distillation.\n",
    "\n",
    "  Returns:\n",
    "    A scalar Tensor containing the distillation loss.\n",
    "  \"\"\"\n",
    " # your code start from here for step 3\n",
    "  soft_targets = tf.nn.softmax(teacher_logits/temperature, axis = -1)\n",
    "\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cd_MUrhLnQA"
   },
   "source": [
    "# TA loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1698677469254,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "WUOeJXWewrNW"
   },
   "outputs": [],
   "source": [
    "def compute_ta_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  ta_subclass_logits = ta_model(images, training=True)\n",
    "\n",
    "  # Compute subclass distillation loss between student subclass logits and\n",
    "  # softened teacher subclass targets probabilities.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  teacher_subclass_logits = teacher_model(images, training=False)\n",
    "  distillation_loss_value = distillation_loss(teacher_subclass_logits,\n",
    "                ta_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  # Compute cross-entropy loss with hard targets.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  cross_entropy_loss_value = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels, ta_subclass_logits))\n",
    "\n",
    "  return ALPHA*distillation_loss_value + (1 - ALPHA)*cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sYPts1wP52D"
   },
   "source": [
    "# Student loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1698677471535,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "GVDn0CHvP3oJ"
   },
   "outputs": [],
   "source": [
    "def compute_student_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  student_subclass_logits = student_model(images, training=True)\n",
    "\n",
    "  # Compute subclass distillation loss between student subclass logits and\n",
    "  # softened teacher subclass targets probabilities.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  ta_subclass_logits = ta_model(images, training = False)\n",
    "  distillation_loss_value = distillation_loss(ta_subclass_logits,\n",
    "                student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  # Compute cross-entropy loss with hard targets.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  cross_entropy_loss_value = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels, student_subclass_logits))\n",
    "\n",
    "  return ALPHA*distillation_loss_value + (1 - ALPHA)*cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLJZRUBJLuoo"
   },
   "source": [
    "# Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1698677474575,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "NBZ3qhk17s8O"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_F1_score(true_binary, pred_binary):\n",
    "  \"\"\"Compute F1 score between true labels and prediction.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Number of correctly classified images.\n",
    "  \"\"\"\n",
    "  return f1_score(true_binary, pred_binary)\n",
    "\n",
    "def get_binary_labels_batch(model, images, labels):\n",
    "  class_logits = model(images, training=False)\n",
    "  pred_binary = tf.argmax(class_logits, -1).numpy()\n",
    "  true_binary = tf.argmax(labels, -1).numpy()\n",
    "  return true_binary, pred_binary\n",
    "\n",
    "@tf.function\n",
    "def compute_num_correct(model, images, labels):\n",
    "  \"\"\"Compute number of correctly classified images in a batch.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Number of correctly classified images.\n",
    "  \"\"\"\n",
    "  class_logits = model(images, training=False)\n",
    "  return tf.reduce_sum(\n",
    "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
    "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn, lr, num_epochs):\n",
    "  \"\"\"Perform training and evaluation for a given model.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    compute_loss_fn: A function that computes the training loss given the\n",
    "      images, and labels.\n",
    "  \"\"\"\n",
    "\n",
    "  # your code start from here for step 4\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "  f1_score_epochs, class_acc_epochs = [], []\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Run training.\n",
    "    print('Epoch {}: '.format(epoch), end='')\n",
    "    for images, labels in mhist_train:\n",
    "      with tf.GradientTape() as tape:\n",
    "         # your code start from here for step 4\n",
    "\n",
    "        loss_value = compute_loss_fn(images, labels)\n",
    "\n",
    "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Run evaluation.\n",
    "    true_binary, pred_binary = [], []\n",
    "    num_correct = 0\n",
    "    num_total = builder.info.splits['test'].num_examples\n",
    "    for images, labels in mhist_test:\n",
    "      # your code start from here for step 4\n",
    "      true_bin_batch, pred_bin_batch = get_binary_labels_batch(model, images, labels)\n",
    "      true_binary.append(true_bin_batch)\n",
    "      pred_binary.append(pred_bin_batch)\n",
    "\n",
    "      num_correct_batch, pred_digit, true_digit = compute_num_correct(model, images, labels)\n",
    "      num_correct += num_correct_batch\n",
    "\n",
    "    ## Estimate F1 score\n",
    "    true_binary = np.concatenate(true_binary)\n",
    "    pred_binary = np.concatenate(pred_binary)\n",
    "    F1_score = compute_F1_score(true_binary, pred_binary)\n",
    "    f1_score_epochs.append(F1_score)\n",
    "    class_acc_epochs.append(num_correct / num_total * 100)\n",
    "    print(\"F1 score: {:.2f} - Class accuracy: {:.2f}%\".format(F1_score,\n",
    "                                            num_correct / num_total * 100))\n",
    "  return f1_score_epochs, class_acc_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQL1lJdaRPT1"
   },
   "source": [
    "# Training TA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2007808,
     "status": "ok",
     "timestamp": 1698679484632,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "LgKPPXQzQqSx",
    "outputId": "ad2a64d7-65bb-49db-8ba7-77f887b3f144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL Training for TA Model\n",
      "Epoch 1: F1 score: 0.74 - Class accuracy: 78.20%\n",
      "Epoch 2: F1 score: 0.67 - Class accuracy: 79.22%\n",
      "Epoch 3: F1 score: 0.62 - Class accuracy: 78.20%\n",
      "Epoch 4: F1 score: 0.74 - Class accuracy: 80.55%\n",
      "Epoch 5: F1 score: 0.59 - Class accuracy: 76.56%\n",
      "Epoch 6: F1 score: 0.73 - Class accuracy: 80.04%\n",
      "Epoch 7: F1 score: 0.73 - Class accuracy: 80.66%\n",
      "Epoch 8: F1 score: 0.65 - Class accuracy: 78.61%\n",
      "Epoch 9: F1 score: 0.57 - Class accuracy: 76.05%\n",
      "Epoch 10: F1 score: 0.74 - Class accuracy: 81.06%\n",
      "FINE TUNING Training for TA Model\n",
      "Epoch 1: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 2681 calls to <function _BaseOptimizer._update_step_xla at 0x79827bd071c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 2682 calls to <function _BaseOptimizer._update_step_xla at 0x79827bd071c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.75 - Class accuracy: 82.29%\n",
      "Epoch 2: F1 score: 0.76 - Class accuracy: 82.19%\n",
      "Epoch 3: F1 score: 0.78 - Class accuracy: 82.60%\n",
      "Epoch 4: F1 score: 0.77 - Class accuracy: 83.93%\n",
      "Epoch 5: F1 score: 0.78 - Class accuracy: 84.34%\n",
      "Epoch 6: F1 score: 0.79 - Class accuracy: 84.85%\n",
      "Epoch 7: F1 score: 0.74 - Class accuracy: 82.29%\n",
      "Epoch 8: F1 score: 0.75 - Class accuracy: 83.62%\n",
      "Epoch 9: F1 score: 0.71 - Class accuracy: 81.06%\n",
      "Epoch 10: F1 score: 0.78 - Class accuracy: 83.62%\n",
      "Epoch 11: F1 score: 0.75 - Class accuracy: 83.73%\n",
      "Epoch 12: F1 score: 0.77 - Class accuracy: 84.34%\n",
      "Epoch 13: F1 score: 0.76 - Class accuracy: 83.42%\n",
      "Epoch 14: F1 score: 0.68 - Class accuracy: 80.25%\n",
      "Epoch 15: F1 score: 0.76 - Class accuracy: 84.34%\n",
      "Epoch 16: F1 score: 0.77 - Class accuracy: 83.83%\n",
      "Epoch 17: F1 score: 0.76 - Class accuracy: 84.34%\n",
      "Epoch 18: F1 score: 0.67 - Class accuracy: 80.04%\n",
      "Epoch 19: F1 score: 0.70 - Class accuracy: 81.06%\n",
      "Epoch 20: F1 score: 0.79 - Class accuracy: 83.21%\n",
      "Epoch 21: F1 score: 0.73 - Class accuracy: 82.70%\n",
      "Epoch 22: F1 score: 0.79 - Class accuracy: 84.34%\n",
      "Epoch 23: F1 score: 0.79 - Class accuracy: 84.65%\n",
      "Epoch 24: F1 score: 0.80 - Class accuracy: 84.85%\n",
      "Epoch 25: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.76 - Class accuracy: 83.32%\n",
      "Average F1 score and class accuracy for the last five epochs\n",
      "F1 score = 0.7728414454765564\n",
      "class accuracy = 83.97134\n"
     ]
    }
   ],
   "source": [
    "if train_ta == True:\n",
    "  # Initial training for the TA  model\n",
    "  print('INITIAL Training for TA Model')\n",
    "  f1_hist, acc_hist = train_and_evaluate(ta_model, compute_ta_loss, 1e-3, INITIAL_EPOCHS)\n",
    "\n",
    "  ## Fine Tuning Step (make all layers trainable)\n",
    "  ta_base.trainable = True\n",
    "  # Fine tune only top layers (e.g. above 100)\n",
    "  fine_tune_at = 280\n",
    "  for layer in ta_base.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "  print('FINE TUNING Training for TA Model')\n",
    "  f1_hist, acc_hist = train_and_evaluate(ta_model, compute_ta_loss, 0.1*1e-3, FINE_TUNE_EPOCHS)\n",
    "\n",
    "  print('Average F1 score and class accuracy for the last five epochs')\n",
    "  print('F1 score = ' + str(np.mean(f1_hist[-5:])))\n",
    "  print('class accuracy = ' + str(np.mean(acc_hist[-5:])))\n",
    "\n",
    "  ## Save TA Model\n",
    "  if run_type == 'local':\n",
    "    ta_model.save('TA_Model_Task2_' + model + '.h5')\n",
    "    np.savez('TA_Model_Task2_hist_' + model+ '.npz', f1_hist = f1_hist, acc_hist = acc_hist)\n",
    "  elif run_type == 'colab':\n",
    "    ta_model.save('/content/drive/My Drive/Colab Notebooks/TA_Model_Task2_' + model +'.h5')\n",
    "    np.savez('/content/drive/My Drive/Colab Notebooks/TA_Model_Task2_hist_' + model +'.npz',\n",
    "             f1_hist = f1_hist, acc_hist = acc_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1698679672000,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "nVIhEdqah7OA"
   },
   "outputs": [],
   "source": [
    "if train_ta == False:\n",
    "  print('Load pre-trained teacher assistant model ' + model)\n",
    "  if run_type == 'local':\n",
    "    teacher_model = load_model('TA_Model_Task2_' + model + '.h5')\n",
    "    hist = np.load('TA_Model_Task2_hist_' + model + '.npz')\n",
    "  else:\n",
    "    teacher_model = load_model('/content/drive/My Drive/Colab Notebooks/TA_Model_Task2_' + model +'.h5')\n",
    "    hist = np.load('/content/drive/My Drive/Colab Notebooks/TA_Model_Task2_hist_' + model + '.npz')\n",
    "\n",
    "  print('Average F1 score and class accuracy for the last five epochs')\n",
    "  print('Teacher test F1 score = ' + str(np.mean(hist['f1_hist'][-5:])))\n",
    "  print('Teacher test class accuracy = ' + str(np.mean(hist['acc_hist'][-5:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1574776,
     "status": "ok",
     "timestamp": 1698681251474,
     "user": {
      "displayName": "jackleen nagy",
      "userId": "05100877981613736414"
     },
     "user_tz": 240
    },
    "id": "tC0TfVsrMwox",
    "outputId": "87399fe3-6b39-4b28-c0c5-9044b6509391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL Training for Student Model\n",
      "Epoch 1: F1 score: 0.70 - Class accuracy: 78.20%\n",
      "Epoch 2: F1 score: 0.40 - Class accuracy: 71.24%\n",
      "Epoch 3: F1 score: 0.63 - Class accuracy: 77.69%\n",
      "Epoch 4: F1 score: 0.69 - Class accuracy: 78.71%\n",
      "Epoch 5: F1 score: 0.61 - Class accuracy: 76.77%\n",
      "Epoch 6: F1 score: 0.57 - Class accuracy: 75.74%\n",
      "Epoch 7: F1 score: 0.67 - Class accuracy: 77.38%\n",
      "Epoch 8: F1 score: 0.60 - Class accuracy: 76.56%\n",
      "Epoch 9: F1 score: 0.47 - Class accuracy: 72.06%\n",
      "Epoch 10: F1 score: 0.40 - Class accuracy: 70.52%\n",
      "FINE TUNING Training for Student Model\n",
      "Epoch 1: F1 score: 0.22 - Class accuracy: 66.94%\n",
      "Epoch 2: F1 score: 0.57 - Class accuracy: 76.36%\n",
      "Epoch 3: F1 score: 0.51 - Class accuracy: 74.51%\n",
      "Epoch 4: F1 score: 0.69 - Class accuracy: 80.55%\n",
      "Epoch 5: F1 score: 0.75 - Class accuracy: 82.19%\n",
      "Epoch 6: F1 score: 0.65 - Class accuracy: 79.32%\n",
      "Epoch 7: F1 score: 0.70 - Class accuracy: 81.47%\n",
      "Epoch 8: F1 score: 0.76 - Class accuracy: 82.91%\n",
      "Epoch 9: F1 score: 0.55 - Class accuracy: 76.66%\n",
      "Epoch 10: F1 score: 0.78 - Class accuracy: 84.44%\n",
      "Epoch 11: F1 score: 0.58 - Class accuracy: 77.69%\n",
      "Epoch 12: F1 score: 0.77 - Class accuracy: 84.24%\n",
      "Epoch 13: F1 score: 0.67 - Class accuracy: 80.66%\n",
      "Epoch 14: F1 score: 0.76 - Class accuracy: 84.03%\n",
      "Epoch 15: F1 score: 0.77 - Class accuracy: 81.99%\n",
      "Epoch 16: F1 score: 0.71 - Class accuracy: 81.99%\n",
      "Epoch 17: F1 score: 0.76 - Class accuracy: 83.62%\n",
      "Epoch 18: F1 score: 0.79 - Class accuracy: 85.26%\n",
      "Epoch 19: F1 score: 0.79 - Class accuracy: 84.95%\n",
      "Epoch 20: F1 score: 0.77 - Class accuracy: 84.14%\n",
      "Epoch 21: F1 score: 0.81 - Class accuracy: 85.98%\n",
      "Epoch 22: F1 score: 0.71 - Class accuracy: 82.09%\n",
      "Epoch 23: F1 score: 0.75 - Class accuracy: 83.93%\n",
      "Epoch 24: F1 score: 0.68 - Class accuracy: 80.96%\n",
      "Epoch 25: F1 score: 0.77 - Class accuracy: 85.06%\n",
      "Average F1 score and class accuracy for the last five epochs\n",
      "F1 score = 0.7426540701303138\n",
      "class accuracy = 83.60287\n"
     ]
    }
   ],
   "source": [
    "# Initial training for the student model\n",
    "print('INITIAL Training for Student Model')\n",
    "f1_hist, acc_hist = train_and_evaluate(student_model, compute_student_loss, 1e-3, INITIAL_EPOCHS)\n",
    "\n",
    "## Fine Tuning Step (make all layers trainable)\n",
    "mobilenet_base.trainable = True\n",
    "# Fine tune only top layers (e.g. above 80)\n",
    "fine_tune_at = 80\n",
    "for layer in mobilenet_base.layers[:fine_tune_at]:\n",
    "  layer.trainable = False\n",
    "print('FINE TUNING Training for Student Model')\n",
    "f1_hist, acc_hist = train_and_evaluate(student_model, compute_student_loss, 0.1*1e-3, FINE_TUNE_EPOCHS)\n",
    "\n",
    "\n",
    "print('Average F1 score and class accuracy for the last five epochs')\n",
    "print('F1 score = ' + str(np.mean(f1_hist[-5:])))\n",
    "print('class accuracy = ' + str(np.mean(acc_hist[-5:])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
